{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40164d6-1ff8-4ccc-851b-b04167b3671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9049e5e-006d-4b6d-992f-afea889be952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee29a60-7a89-47e2-a1a1-393fe01e023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordCount(data):\n",
    "\n",
    "    # 27 keys (26 letters and space)\n",
    "    keys = list(string.ascii_lowercase + \" \")\n",
    "    \n",
    "    wordcount = {}\n",
    "    for letter in data:\n",
    "        if letter not in wordcount:\n",
    "            wordcount[letter] = 0\n",
    "        wordcount[letter] += 1    \n",
    "\n",
    "    updated_wordcount = {key: wordcount.get(key,0) for key in keys}\n",
    "\n",
    "    return updated_wordcount\n",
    "\n",
    "def GetData(file_name_list):\n",
    "\n",
    "    with open(\"languageID/\" + file_name_list[0], encoding = \"utf-8\") as file:\n",
    "        temp_data = file.read()\n",
    "    data = pd.DataFrame(WordCount(temp_data), index = [0])\n",
    "\n",
    "    for i in range(1, len(file_name_list)):\n",
    "        with open(\"languageID/\" + file_name_list[i], encoding = \"utf-8\") as file:\n",
    "            temp_data = file.read()\n",
    "        word_doc = pd.DataFrame(WordCount(temp_data), index = [i])\n",
    "        data = pd.concat([data, word_doc], axis=0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def GetProb(counts, alpha = 1/2):\n",
    "    prob = (counts + alpha) / (sum(counts) + len(counts) * alpha)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93004c08-c14c-4b4e-89b3-b53a3e6b6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files in folder\n",
    "\n",
    "file_name_list = [f for f in os.listdir('languageID')]\n",
    "\n",
    "doc_nums, y = [], []\n",
    "for i in range(len(file_name_list)):\n",
    "    temp = file_name_list[i].split(\".\")[0]\n",
    "    doc_nums.append(temp[1:])\n",
    "    y.append(temp[0])\n",
    "\n",
    "data = GetData(file_name_list)\n",
    "data = pd.concat([pd.DataFrame({'label':y, 'doc':doc_nums}), data], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535c0637-ecec-4dc6-bae2-e8bf189d5b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "j    0.333333\n",
      "s    0.333333\n",
      "e    0.333333\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# prior prob\n",
    "counts = data['label'].value_counts()\n",
    "prior_prob = GetProb(counts)\n",
    "log_prior_prob = np.log(prior_prob)\n",
    "print(prior_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a551827-0d38-4f22-8b6c-59834f22a816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>probability w/smoothing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.060189</td>\n",
       "      <td>0.060169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.011135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.021496</td>\n",
       "      <td>0.02151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.021959</td>\n",
       "      <td>0.021973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.10543</td>\n",
       "      <td>0.105369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.018917</td>\n",
       "      <td>0.018933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.017479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0.047225</td>\n",
       "      <td>0.047216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.055427</td>\n",
       "      <td>0.055411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0.02897</td>\n",
       "      <td>0.028977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>0.020504</td>\n",
       "      <td>0.020519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0.05794</td>\n",
       "      <td>0.057922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0.064488</td>\n",
       "      <td>0.064464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.016734</td>\n",
       "      <td>0.016752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.05384</td>\n",
       "      <td>0.053825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0.066208</td>\n",
       "      <td>0.066182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>0.080164</td>\n",
       "      <td>0.080126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>0.026655</td>\n",
       "      <td>0.026664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>0.00926</td>\n",
       "      <td>0.009285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.015477</td>\n",
       "      <td>0.015496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.013824</td>\n",
       "      <td>0.013844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.179377</td>\n",
       "      <td>0.17925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  probability probability w/smoothing\n",
       "a    0.060189                0.060169\n",
       "b    0.011112                0.011135\n",
       "c    0.021496                 0.02151\n",
       "d    0.021959                0.021973\n",
       "e     0.10543                0.105369\n",
       "f    0.018917                0.018933\n",
       "g    0.017461                0.017479\n",
       "h    0.047225                0.047216\n",
       "i    0.055427                0.055411\n",
       "j    0.001389                0.001421\n",
       "k    0.003704                0.003734\n",
       "l     0.02897                0.028977\n",
       "m    0.020504                0.020519\n",
       "n     0.05794                0.057922\n",
       "o    0.064488                0.064464\n",
       "p    0.016734                0.016752\n",
       "q    0.000529                0.000562\n",
       "r     0.05384                0.053825\n",
       "s    0.066208                0.066182\n",
       "t    0.080164                0.080126\n",
       "u    0.026655                0.026664\n",
       "v     0.00926                0.009285\n",
       "w    0.015477                0.015496\n",
       "x    0.001124                0.001156\n",
       "y    0.013824                0.013844\n",
       "z    0.000595                0.000628\n",
       "     0.179377                 0.17925"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata = data[data['label'] == \"e\"]\n",
    "subdata = pd.concat([subdata.sort_values(by='doc')[0:2], subdata.sort_values(by='doc')[12:]]) # training\n",
    "temp = subdata.sum()[2:]\n",
    "\n",
    "q32 = pd.concat([pd.DataFrame(GetProb(temp, alpha = 0)), pd.DataFrame(GetProb(temp, alpha = 1/2))], axis = 1)\n",
    "q32.columns = ['probability', 'probability w/smoothing']\n",
    "q32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ed9c4c-ae90-40d8-bd06-3cc29f79e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j_probability</th>\n",
       "      <th>j_probability w/smoothing</th>\n",
       "      <th>s_probability</th>\n",
       "      <th>s_probability w/smoothing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.131855</td>\n",
       "      <td>0.131766</td>\n",
       "      <td>0.104617</td>\n",
       "      <td>0.10456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.008233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>0.037526</td>\n",
       "      <td>0.037526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.039748</td>\n",
       "      <td>0.039746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.060227</td>\n",
       "      <td>0.060205</td>\n",
       "      <td>0.113875</td>\n",
       "      <td>0.113811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.008579</td>\n",
       "      <td>0.008603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0.01399</td>\n",
       "      <td>0.014012</td>\n",
       "      <td>0.00716</td>\n",
       "      <td>0.007184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0.031757</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.09709</td>\n",
       "      <td>0.097033</td>\n",
       "      <td>0.04987</td>\n",
       "      <td>0.04986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>0.057429</td>\n",
       "      <td>0.057409</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.052956</td>\n",
       "      <td>0.052943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>0.039801</td>\n",
       "      <td>0.039799</td>\n",
       "      <td>0.025799</td>\n",
       "      <td>0.025809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0.056729</td>\n",
       "      <td>0.056711</td>\n",
       "      <td>0.054191</td>\n",
       "      <td>0.054177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0.091214</td>\n",
       "      <td>0.091163</td>\n",
       "      <td>0.072522</td>\n",
       "      <td>0.072492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>0.024267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.042809</td>\n",
       "      <td>0.042804</td>\n",
       "      <td>0.059314</td>\n",
       "      <td>0.059295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0.04218</td>\n",
       "      <td>0.042175</td>\n",
       "      <td>0.065794</td>\n",
       "      <td>0.06577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>0.057009</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>0.035613</td>\n",
       "      <td>0.035614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>0.070649</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.033702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.005889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.019726</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.002498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.01413</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.007863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.002683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.123531</td>\n",
       "      <td>0.123449</td>\n",
       "      <td>0.168374</td>\n",
       "      <td>0.168265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  j_probability j_probability w/smoothing s_probability  \\\n",
       "a      0.131855                  0.131766      0.104617   \n",
       "b      0.010842                  0.010867      0.008209   \n",
       "c      0.005456                  0.005486      0.037526   \n",
       "d      0.017208                  0.017226      0.039748   \n",
       "e      0.060227                  0.060205      0.113875   \n",
       "f      0.003847                  0.003879      0.008579   \n",
       "g       0.01399                  0.014012       0.00716   \n",
       "h      0.031757                  0.031762      0.004506   \n",
       "i       0.09709                  0.097033       0.04987   \n",
       "j      0.002308                  0.002341      0.006604   \n",
       "k      0.057429                  0.057409      0.000247   \n",
       "l      0.001399                  0.001433      0.052956   \n",
       "m      0.039801                  0.039799      0.025799   \n",
       "n      0.056729                  0.056711      0.054191   \n",
       "o      0.091214                  0.091163      0.072522   \n",
       "p      0.000839                  0.000874      0.024256   \n",
       "q       0.00007                  0.000105      0.007653   \n",
       "r      0.042809                  0.042804      0.059314   \n",
       "s       0.04218                  0.042175      0.065794   \n",
       "t      0.057009                   0.05699      0.035613   \n",
       "u      0.070649                  0.070617        0.0337   \n",
       "v       0.00021                  0.000245      0.005863   \n",
       "w      0.019726                  0.019742      0.000062   \n",
       "x           0.0                  0.000035      0.002469   \n",
       "y       0.01413                  0.014151      0.007839   \n",
       "z      0.007694                  0.007722      0.002654   \n",
       "       0.123531                  0.123449      0.168374   \n",
       "\n",
       "  s_probability w/smoothing  \n",
       "a                   0.10456  \n",
       "b                  0.008233  \n",
       "c                  0.037526  \n",
       "d                  0.039746  \n",
       "e                  0.113811  \n",
       "f                  0.008603  \n",
       "g                  0.007184  \n",
       "h                  0.004533  \n",
       "i                   0.04986  \n",
       "j                  0.006629  \n",
       "k                  0.000278  \n",
       "l                  0.052943  \n",
       "m                  0.025809  \n",
       "n                  0.054177  \n",
       "o                  0.072492  \n",
       "p                  0.024267  \n",
       "q                  0.007678  \n",
       "r                  0.059295  \n",
       "s                   0.06577  \n",
       "t                  0.035614  \n",
       "u                  0.033702  \n",
       "v                  0.005889  \n",
       "w                  0.000093  \n",
       "x                  0.002498  \n",
       "y                  0.007863  \n",
       "z                  0.002683  \n",
       "                   0.168265  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata = data[data['label'] == \"j\"]\n",
    "subdata = pd.concat([subdata.sort_values(by='doc')[0:2], subdata.sort_values(by='doc')[12:]]) # training\n",
    "temp = subdata.sum()[2:]\n",
    "q33_j = pd.concat([pd.DataFrame(GetProb(temp, alpha = 0)), pd.DataFrame(GetProb(temp, alpha = 1/2))], axis = 1)\n",
    "q33_j.columns = ['j_probability', 'j_probability w/smoothing']\n",
    "\n",
    "subdata = data[data['label'] == \"s\"]\n",
    "subdata = pd.concat([subdata.sort_values(by='doc')[0:2], subdata.sort_values(by='doc')[12:]]) # training\n",
    "temp = subdata.sum()[2:]\n",
    "q33_s = pd.concat([pd.DataFrame(GetProb(temp, alpha = 0)), pd.DataFrame(GetProb(temp, alpha = 1/2))], axis = 1)\n",
    "q33_s.columns = ['s_probability', 's_probability w/smoothing']\n",
    "\n",
    "pd.concat([q33_j, q33_s], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560e0d4e-4530-4278-a10f-97f4e44370c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>164</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>57</td>\n",
       "      <td>311</td>\n",
       "      <td>55</td>\n",
       "      <td>51</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>64</td>\n",
       "      <td>139</td>\n",
       "      <td>182</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>141</td>\n",
       "      <td>186</td>\n",
       "      <td>225</td>\n",
       "      <td>65</td>\n",
       "      <td>31</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a   b   c   d    e   f   g    h    i  j  k   l   m    n    o   p  q  \\\n",
       "42  164  32  53  57  311  55  51  140  140  3  6  85  64  139  182  53  3   \n",
       "\n",
       "      r    s    t   u   v   w  x   y  z       \n",
       "42  141  186  225  65  31  47  4  38  2  498  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "x = data[(data['label'] == 'e') & (data['doc']== '10')].iloc[:,2:]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a262fd-5962-4546-9992-7e15af39403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_e = q32.iloc[:,1].astype(float)\n",
    "p_j = q33_j.iloc[:,1].astype(float)\n",
    "p_s = q33_s.iloc[:,1].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c3f5c7-e22e-4c55-b1e4-152582a56c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-7841.865447060635, -8771.433079075032, -8467.282044010557], 'e')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CondProb(p_e, p_j, p_s, x):\n",
    "    log_prob = [np.sum(np.log(p_e) * x), np.sum(np.log(p_j) * x), np.sum(np.log(p_s) * x)]\n",
    "    pred_label = np.array([\"e\",\"j\",\"s\"])[np.argmax(log_prob)]\n",
    "    return log_prob, pred_label\n",
    "\n",
    "# predicted probability based on bayes rule (prior not included due to the same prior probs)\n",
    "CondProb(p_e, p_j, p_s, x.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e2b3eb-0682-44ad-b4f0-ab34699bc7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.,  0.],\n",
       "       [ 0., 10.,  0.],\n",
       "       [ 0.,  0., 10.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "subdata = data[data['label'] == \"e\"]\n",
    "subdata = subdata.sort_values(by='doc')[2:12]\n",
    "subdata2 = data[data['label'] == \"j\"]\n",
    "subdata = pd.concat([subdata, subdata2.sort_values(by='doc')[2:12]])\n",
    "subdata3 = data[data['label'] == \"s\"]\n",
    "subdata = pd.concat([subdata, subdata3.sort_values(by='doc')[2:12]])\n",
    "\n",
    "pred_labels = []\n",
    "for i in range(len(subdata)):\n",
    "    pred_labels.append(CondProb(p_e, p_j, p_s, subdata.iloc[i][2:].values)[1])    \n",
    "\n",
    "true_labels = subdata['label'].values\n",
    "\n",
    "confusion_matrix = np.zeros((3, 3))\n",
    "for true, pred in zip(true_labels, pred_labels):\n",
    "    if pred == \"e\" and true == \"e\":\n",
    "        confusion_matrix[0,0] += 1\n",
    "    elif pred == \"e\" and true == \"j\":\n",
    "        confusion_matrix[0,1] += 1        \n",
    "    elif pred == \"e\" and true == \"s\":\n",
    "        confusion_matrix[0,2] += 1                \n",
    "    elif pred == \"j\" and true == \"e\":\n",
    "        confusion_matrix[1,0] += 1        \n",
    "    elif pred == \"j\" and true == \"j\":\n",
    "        confusion_matrix[1,1] += 1        \n",
    "    elif pred == \"j\" and true == \"s\":\n",
    "        confusion_matrix[1,2] += 1        \n",
    "    elif pred == \"s\" and true == \"e\":\n",
    "        confusion_matrix[2,0] += 1        \n",
    "    elif pred == \"s\" and true == \"j\":\n",
    "        confusion_matrix[2,1] += 1                \n",
    "    elif pred == \"s\" and true == \"s\":\n",
    "        confusion_matrix[2,2] += 1                        \n",
    "confusion_matrix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d99718-38b8-4102-a8e4-389141b99e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3043f288-0fe6-4ac5-8902-d4ec84ab1bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joowonlee/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2209adb2-b968-4ec6-8715-2436ea85f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train).astype(int)\n",
    "\n",
    "n, d = X_train.shape\n",
    "d1 = 300\n",
    "k = 10\n",
    "\n",
    "h = 28 # the number of hidden units\n",
    "w1 = np.random.randn(d1, d)\n",
    "w2 = np.random.randn(k, d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd662f2-8cf2-4a5b-9cbf-3b0b72a68d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z) \n",
    "    return e_z / e_z.sum()\n",
    "\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    new_y = np.zeros(10)\n",
    "    new_y[y] = 1\n",
    "    return -np.sum(new_y * np.log(y_hat))\n",
    "\n",
    "# Forward \n",
    "def Forward(x, w1, w2):\n",
    "    z1 = np.dot(w1, x) # d1 x 1\n",
    "    a = sigmoid(z1) \n",
    "    z2 = np.dot(w2, a) # k x 1\n",
    "    y_hat = softmax(z2)\n",
    "    return a, y_hat\n",
    "\n",
    "# Backpropagation\n",
    "def backward(w1, w2, x, y, a, y_hat):\n",
    "    new_y = np.zeros(10)\n",
    "    new_y[int(y)] = 1\n",
    "    delta2 = y_hat - new_y # k x 1\n",
    "    dW2 = np.outer(delta2, a) # k x d1\n",
    "    delta1 = np.dot(w2.T, delta2) * a * (1 - a) # d1 x 1\n",
    "    dW1 = np.outer(delta1, x.T) # d1 x d\n",
    "    return dW1, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06b43aa-85d8-484d-bf2c-cd1e6e0b044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/gq7_t55j62v7jbmngfldbqs80000gn/T/ipykernel_11999/833059606.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2247455.0352960066\n",
      "1 2168605.5805212967\n",
      "2 2080208.6637301706\n",
      "3 2012161.1423346933\n",
      "4 1978001.694119834\n",
      "5 1925016.1456752764\n",
      "6 1924310.0903544633\n",
      "7 1839381.935201968\n",
      "8 1794615.2617308097\n",
      "9 1771583.5590785933\n",
      "10 1810310.202001031\n",
      "11 1722411.6018239565\n",
      "12 1831121.9897148276\n",
      "13 1797037.2044657234\n",
      "14 1832087.2451497102\n",
      "15 1765132.406338417\n",
      "16 1746577.3123243582\n",
      "17 1769657.1135487345\n",
      "18 1762864.3403374064\n",
      "19 1721454.4593111076\n",
      "20 1794038.5775256867\n",
      "21 1683468.2286407095\n",
      "22 1667981.78354452\n",
      "23 1683082.9642062369\n",
      "24 1664925.8461422806\n",
      "25 1653817.9818285399\n",
      "26 1618835.5749674616\n",
      "27 1610819.467529964\n",
      "28 1606672.0995883066\n",
      "29 1630864.9433707986\n",
      "30 1612553.8804299082\n",
      "31 1766324.436525578\n",
      "32 1734335.764572984\n",
      "33 1636188.1274919917\n",
      "34 1619034.5072924842\n",
      "35 1604520.8298584297\n",
      "36 1627886.1739230151\n",
      "37 1588230.880282215\n",
      "38 1558332.3496896815\n",
      "39 1549484.0633444532\n",
      "40 1527144.4128712416\n",
      "41 1512454.670170233\n",
      "42 1506100.196114059\n",
      "43 1504383.2997971245\n",
      "44 1497002.2911285611\n",
      "45 1515892.0135821346\n",
      "46 1518536.2367367384\n",
      "47 1516458.1303343887\n",
      "48 1508106.9056886984\n",
      "49 1521242.606593551\n",
      "50 1518998.6389927755\n",
      "51 1509777.3634486045\n",
      "52 1487249.8562568\n",
      "53 1511250.0571527963\n",
      "54 1512766.8825094819\n",
      "55 1504691.6059358297\n",
      "56 1461938.8542546886\n",
      "57 1445220.5341383421\n",
      "58 1433789.2265949924\n",
      "59 1456838.095579121\n",
      "60 1456323.5058609692\n",
      "61 1451327.2754744843\n",
      "62 1406163.4001643672\n",
      "63 1416770.5986485195\n",
      "64 1410036.4579586887\n",
      "65 1426888.841033908\n",
      "66 1403560.0996077897\n",
      "67 1425847.9823926967\n",
      "68 1424424.7854388792\n",
      "69 1469831.7602658584\n",
      "70 1471800.120000423\n",
      "71 1463628.58720502\n",
      "72 1447944.7454409534\n",
      "73 1497384.5804327752\n",
      "74 1448544.5677984764\n",
      "75 1470180.8506945893\n",
      "76 1437633.6417305502\n",
      "77 1420970.36111332\n",
      "78 1403266.8785319799\n",
      "79 1478320.7996427324\n",
      "80 1484899.9494874745\n",
      "81 1484943.552501363\n",
      "82 1499462.4584761835\n",
      "83 1483289.6317095193\n",
      "84 1458187.7313286383\n",
      "85 1517578.6960577066\n",
      "86 1482278.9609961463\n",
      "87 1455493.6533957457\n",
      "88 1451826.1652048468\n",
      "89 1472092.8008559246\n",
      "90 1477607.428484585\n",
      "91 1453418.6375165437\n",
      "92 1425903.5899347803\n",
      "93 1414921.2559379013\n",
      "94 1418806.6500651704\n",
      "95 1423091.5631950025\n",
      "96 1447187.8200774384\n",
      "97 1434868.329500272\n",
      "98 1451952.5799269886\n",
      "99 1465373.118607481\n",
      "100 1473527.075591069\n",
      "101 1426651.8338611037\n",
      "102 1427395.0870015814\n",
      "103 1413791.4515075302\n",
      "104 1447123.395047734\n",
      "105 1473011.0037177838\n",
      "106 1546454.5939824989\n",
      "107 1553583.3734433514\n",
      "108 1533392.2705301687\n",
      "109 1611256.988067013\n",
      "110 1569883.3132570302\n",
      "111 1551096.765274783\n",
      "112 1517652.3245117\n",
      "113 1534780.5884574996\n",
      "114 1482548.1052276283\n",
      "115 1437144.8155010298\n",
      "116 1430077.7384758391\n",
      "117 1435857.7235918865\n",
      "118 1437451.0369188262\n",
      "119 1488503.4945941726\n",
      "120 1505753.3463589065\n",
      "121 1515609.731968029\n",
      "122 1527926.672617332\n",
      "123 1517789.7083901707\n",
      "124 1497329.6254574407\n",
      "125 1490881.9594980772\n",
      "126 1534115.608124876\n",
      "127 1519738.8808576197\n",
      "128 1472368.741701746\n",
      "129 1444318.7019242037\n",
      "130 1456850.7109372218\n",
      "131 1446903.1907422645\n",
      "132 1399024.3911382328\n",
      "133 1424265.037311681\n",
      "134 1391836.3046718584\n",
      "135 1406044.746992004\n",
      "136 1424613.6340535048\n",
      "137 1419126.0257575482\n",
      "138 1433384.7039270906\n",
      "139 1417813.0711572245\n",
      "140 1392884.7035289702\n",
      "141 1428490.4713950462\n",
      "142 1380075.8372349953\n",
      "143 1382820.366241541\n",
      "144 1392362.3401184333\n",
      "145 1371388.3609962948\n",
      "146 1349659.374135545\n",
      "147 1321741.8590913985\n",
      "148 1315084.671230524\n",
      "149 1344545.376749273\n",
      "150 1344028.391516812\n",
      "151 1340189.0557983841\n",
      "152 1335154.6108930383\n",
      "153 1357212.1546623954\n",
      "154 1332326.3008826931\n",
      "155 1324639.1292640097\n",
      "156 1339961.58233321\n",
      "157 1335691.217366699\n",
      "158 1297579.1094874046\n",
      "159 1296718.2027270352\n",
      "160 1323455.91632772\n",
      "161 1327249.9049706568\n",
      "162 1318203.7008772658\n",
      "163 1326277.708265031\n",
      "164 1337358.6315933487\n",
      "165 1350791.0897272676\n",
      "166 1313151.615794658\n",
      "167 1308357.122705581\n",
      "168 1314011.401290865\n",
      "169 1315640.9923225194\n",
      "170 1385617.6383969602\n",
      "171 1421117.7332588148\n",
      "172 1364220.3264218003\n",
      "173 1380586.8090587258\n",
      "174 1408892.8915101397\n",
      "175 1412320.500154336\n",
      "176 1406112.7494753925\n",
      "177 1370209.1262816982\n",
      "178 1408891.4744959243\n",
      "179 1380227.7099053527\n",
      "180 1332304.4535096053\n",
      "181 1339698.7138839085\n",
      "182 1332541.8794737314\n",
      "183 1344787.3887839094\n",
      "184 1306871.791960857\n",
      "185 1324517.2411266204\n",
      "186 1317969.4368674206\n",
      "187 1327068.2628670444\n",
      "188 1352258.6368745246\n",
      "189 1337544.4869648833\n",
      "190 1346595.206100264\n",
      "191 1372364.4801272945\n",
      "192 1365911.430043273\n",
      "193 1421014.9601940946\n",
      "194 1418717.3035832935\n",
      "195 1394794.1814574178\n",
      "196 1394311.5887723274\n",
      "197 1361959.3076549722\n",
      "198 1353876.7388390359\n",
      "199 1352717.7452471296\n"
     ]
    }
   ],
   "source": [
    "n_iter = 200\n",
    "\n",
    "# Pick a learning rate 'alpha'\n",
    "learning_rate = 0.1\n",
    "\n",
    "# minibatch size\n",
    "batch = 64\n",
    "\n",
    "# Obtain initial cost\n",
    "loss_old = 1000000\n",
    "diff_loss = 1\n",
    "track_loss = []\n",
    "\n",
    "i = 0\n",
    "while i < n_iter and loss_old > 1 and diff_loss > 0.0001:\n",
    "\n",
    "    # Shuffle the dataset for stochastic gradient descent\n",
    "    shuffle_indices = np.random.permutation(n)\n",
    "    sum_dW1, sum_dW2 = 0, 0\n",
    "    \n",
    "    for j in shuffle_indices[0:batch]:\n",
    "        \n",
    "        x = np.array(X_train.iloc[j])\n",
    "        y = y_train[j]\n",
    "\n",
    "        # Forward pass\n",
    "        a, y_hat = Forward(x, w1, w2)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(y, y_hat)\n",
    "\n",
    "        # Backward pass\n",
    "        dW1, dW2 = backward(w1, w2, x, y, a, y_hat)\n",
    "        sum_dW1 += dW1\n",
    "        sum_dW2 += dW2\n",
    "        \n",
    "    # Update weights and biases\n",
    "    w1 -= learning_rate * sum_dW1/batch\n",
    "    w2 -= learning_rate * sum_dW2/batch\n",
    "        \n",
    "    # output\n",
    "    output_a = sigmoid(np.dot(X_train, w1.T)) # n x d1\n",
    "    output = softmax(np.dot(output_a, w2.T)) # n x k\n",
    "    \n",
    "    # loss calculation\n",
    "    loss_new = 0\n",
    "    for j in range(len(y_train)):\n",
    "        loss_new += cross_entropy_loss(y_train[j], output[j])\n",
    "    diff_loss = abs(loss_old - loss_new)\n",
    "    loss_old = loss_new\n",
    "    print(i, loss_old)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        track_loss.append(loss_old)    \n",
    "    \n",
    "    i += 1\n",
    "\n",
    "track_loss.append(loss_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e8a13a-d941-4386-b6a4-47ce077ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 826    1  100   52   29  141   97   45   27   25]\n",
      " [   5 1165  134   17   36   57   27   73   84    2]\n",
      " [  62   94  610  113   75   42  118   91  141   34]\n",
      " [  28   25   99  740   40  199  106   95   73   28]\n",
      " [   5   24   56   27  653   78   70   70   96  216]\n",
      " [  38   25   26  196   81  450  108   77  150  122]\n",
      " [  70   31   76   30  116   57  931   35   32   18]\n",
      " [  32   30   56   43  124   59   11  872   76  200]\n",
      " [  20   47  119  215   51  162   84   94  478   87]\n",
      " [   9   28   21   33  215  106   24  331  100  553]]\n",
      "Test Error: 0.4801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/gq7_t55j62v7jbmngfldbqs80000gn/T/ipykernel_11999/833059606.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "output_a = sigmoid(np.dot(X_test, w1.T)) \n",
    "output = softmax(np.dot(output_a, w2.T)) \n",
    "pred_labels = [np.argmax(output[i]) for i in range(len(output))]\n",
    "\n",
    "conf_m = confusion_matrix(np.array(y_test).astype(int), pred_labels)\n",
    "print(conf_m)\n",
    "print(\"Test Error:\", np.round(1- np.trace(conf_m)/np.sum(conf_m), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2aa2676-d12f-41ee-914f-dc9762a92d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.scatter(np.arange(0, 210, 10), track_loss, label='O', color='red', marker='o', s=100)\n",
    "plt.plot(np.arange(0, 210, 10), track_loss, color='red', linestyle='-')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('# of iterates')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
